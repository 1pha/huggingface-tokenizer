{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Office Hour Baseline Example Code\n",
    "\n",
    "ì˜¤í”¼ìŠ¤ì•„ì›Œ ì‹œê°„ì— ë‹¤ë£¬ ì˜ˆì œ ì½”ë“œ ì˜ˆì œ(?)ì…ë‹ˆë‹¤. íë¦„ì„ ì´í•´í•  ë•Œ ì‹¤ì œë¡œ ì½”ë“œë¥¼ ëŒë ¤ë³´ë©´ì„œ ë”°ë¼ê°€ë©´ ì´í•´ê°€ ë” í¸í•  ê²ƒ ê°™ì•„ ê³µìœ í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ”¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1 = {\n",
    "    \"context\": \"ì„œìš¸ì€ êµ­ì œì ì¸ ë„ì‹œì´ë‹¤. ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ 4ìœ„ì´ë‹¤.\",\n",
    "    \"question\": \"ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ ëª‡ ìœ„ì¸ê°€?\",\n",
    "    \"answers\": {\"answer_start\": [24], \"text\": [\"ì„¸ê³„ 4ìœ„\"]},\n",
    "    \"title\": \"ì˜ˆì œ1\",\n",
    "    \"id\": \"ex-1\",\n",
    "    \"document_id\": 1,\n",
    "}\n",
    "\n",
    "example_2 = {\n",
    "    \"context\": \"ë„·í”Œë¦­ìŠ¤ëŠ” ì „ ì„¸ê³„ 190ê°œêµ­ ì´ìƒì—ì„œ ì´ìš© ì¤‘ì´ë‹¤. ìŠ¤ë¬¼ë‹¤ì„¯ìŠ¤ë¬¼í•˜ë‚˜ ì¬ë°Œë‹¤.\",\n",
    "    \"question\": \"ë„·í”Œë¦­ìŠ¤ëŠ” ëª‡ ê°œêµ­ì—ì„œ ì´ìš© ì¤‘ì¸ê°€?\",\n",
    "    \"answers\": {\"answer_start\": [11], \"text\": [\"190ê°œêµ­\"]},\n",
    "    \"title\": \"ì˜ˆì œ2\",\n",
    "    \"id\": \"ex-2\",\n",
    "    \"document_id\": 2,\n",
    "}\n",
    "\n",
    "examples = {\n",
    "    key: [ex[key] for ex in [example_1, example_2]] for key in example_1.keys()\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìš°ì„  Tokenizerê°€ ë³€í™˜í•˜ëŠ” ê²°ê³¼ë¶€í„° ì‚´í´ë´…ì‹œë‹¤.\n",
    "\n",
    "![image](./assets/01_preprocess_pipeline.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `prepare_train_features`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. `tokenized_examples`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "êµ¬ì„±ëœ examplesì˜ í˜•íƒœëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. context, questionì— listí˜•íƒœë¡œ ë¬¸ìì—´ë“¤ì´ ê°™ì´ ë“¤ì–´ê°€ê³ , answerì—ëŠ” answer_start / textê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë“¤ì–´ê°€ê²Œë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': ['ì„œìš¸ì€ êµ­ì œì ì¸ ë„ì‹œì´ë‹¤. ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ 4ìœ„ì´ë‹¤.',\n",
       "  'ë„·í”Œë¦­ìŠ¤ëŠ” ì „ ì„¸ê³„ 190ê°œêµ­ ì´ìƒì—ì„œ ì´ìš© ì¤‘ì´ë‹¤. ìŠ¤ë¬¼ë‹¤ì„¯ìŠ¤ë¬¼í•˜ë‚˜ ì¬ë°Œë‹¤.'],\n",
       " 'question': ['ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ ëª‡ ìœ„ì¸ê°€?', 'ë„·í”Œë¦­ìŠ¤ëŠ” ëª‡ ê°œêµ­ì—ì„œ ì´ìš© ì¤‘ì¸ê°€?'],\n",
       " 'answers': [{'answer_start': [24], 'text': ['ì„¸ê³„ 4ìœ„']},\n",
       "  {'answer_start': [11], 'text': ['190ê°œêµ­']}],\n",
       " 'title': ['ì˜ˆì œ1', 'ì˜ˆì œ2'],\n",
       " 'id': ['ex-1', 'ex-2'],\n",
       " 'document_id': [1, 2]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "question_column_name = \"question\"\n",
    "context_column_name = \"context\"\n",
    "answer_column_name = \"answers\"\n",
    "\n",
    "max_seq_length = 32\n",
    "doc_stride = 16\n",
    "pad_to_max_length = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_examples = tokenizer(\n",
    "    examples[question_column_name if pad_on_right else context_column_name],\n",
    "    examples[context_column_name if pad_on_right else question_column_name],\n",
    "    truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "    max_length=max_seq_length,\n",
    "    stride=doc_stride,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    # return_token_type_ids=False, # robertaëª¨ë¸ì„ ì‚¬ìš©í•  ê²½ìš° False, bertë¥¼ ì‚¬ìš©í•  ê²½ìš° Trueë¡œ í‘œê¸°í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "    padding=\"max_length\" if pad_to_max_length else False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_examples.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì˜¤í”¼ìŠ¤ì•„ì›Œì—ì„œ ì„¤ëª…í•œëŒ€ë¡œ `tokenizer`ê°€ ì²˜ë¦¬í•œ ê²°ê³¼ê°€ 5ê°œ ì¶œë ¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” `tokenizer`ì—ê²Œ ì£¼ì–´ì§„ parameterì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ ìš°ì„  ì €í¬ ì˜ˆì œë§Œ ì‚´í´ë´…ì‹œë‹¤.\n",
    "- `input_ids`: `tokenizer.encode`ì˜ ê²°ê³¼\n",
    "- `token_type_ids`: questionê³¼ contextì˜ êµ¬ë¶„\n",
    "- `attention_mask`: Encoderì—ê²Œ ì œê³µë  ì˜ˆì •ì´ë¼ `[PAD]` ë¶€ë¶„ ì œì™¸í•˜ë©´ ëª¨ë‘ 1ì…ë‹ˆë‹¤.\n",
    "- `offset_mapping`: ì •ë‹µ í›„ì²˜ë¦¬í•  ë–„ ì‚¬ìš©\n",
    "- `overflow_to_sample_mapping`: í›„ì²˜ë¦¬ì— ì‚¬ìš©2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì›ë˜ ì˜ˆì œì—ì„œëŠ” ë‘ ê°œì˜ ë°ì´í„°ë¥¼ ì…ë ¥í•˜ì˜€ì§€ë§Œ, ì‹¤ì œ ì¸ì½”ë”© ê²°ê³¼ëŠ” 3ê°œê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤. ì˜¤ì•„ì—ì„œ ì„¤ëª…í•œëŒ€ë¡œ example_2ëŠ” ì§€ì •ëœ `max_length` ê¸¸ì´ 32ë¥¼ ì´ˆê³¼í•˜ê¸° ë•Œë¬¸ì— ë¶„ì ˆë˜ì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_examples[\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer`ì˜ ê²°ê³¼ëŠ” `BatchEncoding`ì´ë¼ëŠ” ìë£Œí˜•ì„ ê°€ì§€ëŠ”ë°, ì´ëŠ” ë§ê·¸ëŒ€ë¡œ ì—¬ëŸ¬ ê°œì˜ `Encoding` ìë£Œí˜•ì„ ê°€ì§‘ë‹ˆë‹¤. ì˜ˆì œì—ì„œëŠ” `Encoding` ë‹¨ìœ„ ë˜í•œ ê°™ì´ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "te1 = tokenized_examples[0]\n",
    "te2 = tokenized_examples[1]\n",
    "te3 = tokenized_examples[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Encoding(num_tokens=28, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " tokenizers.Encoding)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te1, type(te1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œë¶€í„° ìš”ì†Œìš”ì†Œ ìì„¸íˆ ì‚´í´ë´…ì‹œë‹¤"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `input_ids`\n",
    "\n",
    "`input_ids`ëŠ” í† í°í™”ëœ ê²°ê³¼ê°€ ì •ìˆ˜í˜• idì— ë§¤í•‘ëœ ê²°ê³¼ì…ë‹ˆë‹¤. `Encoding`ì—ì„œëŠ” `ids`ë¡œ í™•ì¸ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2,\n",
      "  3671,\n",
      "  2079,\n",
      "  9476,\n",
      "  2259,\n",
      "  3665,\n",
      "  1077,\n",
      "  18431,\n",
      "  2116,\n",
      "  35,\n",
      "  3,\n",
      "  3671,\n",
      "  2073,\n",
      "  3854,\n",
      "  31221,\n",
      "  3763,\n",
      "  28674,\n",
      "  18,\n",
      "  3671,\n",
      "  2079,\n",
      "  9476,\n",
      "  2259,\n",
      "  3665,\n",
      "  24,\n",
      "  2090,\n",
      "  28674,\n",
      "  18,\n",
      "  3],\n",
      " [2,\n",
      "  23026,\n",
      "  2259,\n",
      "  1077,\n",
      "  5620,\n",
      "  27135,\n",
      "  3774,\n",
      "  1570,\n",
      "  2179,\n",
      "  2116,\n",
      "  35,\n",
      "  3,\n",
      "  23026,\n",
      "  2259,\n",
      "  1537,\n",
      "  3665,\n",
      "  7175,\n",
      "  2019,\n",
      "  2226,\n",
      "  3658,\n",
      "  27135,\n",
      "  3774,\n",
      "  1570,\n",
      "  28674,\n",
      "  18,\n",
      "  10514,\n",
      "  2062,\n",
      "  2839,\n",
      "  2255,\n",
      "  2266,\n",
      "  8705,\n",
      "  3],\n",
      " [2,\n",
      "  23026,\n",
      "  2259,\n",
      "  1077,\n",
      "  5620,\n",
      "  27135,\n",
      "  3774,\n",
      "  1570,\n",
      "  2179,\n",
      "  2116,\n",
      "  35,\n",
      "  3,\n",
      "  3665,\n",
      "  7175,\n",
      "  2019,\n",
      "  2226,\n",
      "  3658,\n",
      "  27135,\n",
      "  3774,\n",
      "  1570,\n",
      "  28674,\n",
      "  18,\n",
      "  10514,\n",
      "  2062,\n",
      "  2839,\n",
      "  2255,\n",
      "  2266,\n",
      "  8705,\n",
      "  7478,\n",
      "  2062,\n",
      "  18,\n",
      "  3]]\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenized_examples.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3671,\n",
       " 2079,\n",
       " 9476,\n",
       " 2259,\n",
       " 3665,\n",
       " 1077,\n",
       " 18431,\n",
       " 2116,\n",
       " 35,\n",
       " 3,\n",
       " 3671,\n",
       " 2073,\n",
       " 3854,\n",
       " 31221,\n",
       " 3763,\n",
       " 28674,\n",
       " 18,\n",
       " 3671,\n",
       " 2079,\n",
       " 9476,\n",
       " 2259,\n",
       " 3665,\n",
       " 24,\n",
       " 2090,\n",
       " 28674,\n",
       " 18,\n",
       " 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te1.ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‚¬ì‹¤ ì´ë ‡ê²Œ ë³´ë©´ ë­ê°€ ë­”ì§€ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ìš°ë¦¬ê°€ ì•Œì•„ë³¼ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ë¡œ ëŒë ¤ë´…ì‹œë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©í–ˆìœ¼ë‹ˆê¹Œ ë‹¤ì‹œ ë””ì½”ë“œí•˜ë©´ ë©ë‹ˆë‹¤.   \n",
    "ì—¬ëŸ¬ ê°œì˜ ë°ì´í„°ê°€ ë“¤ì–´ìˆì„ ë•ŒëŠ” `tokenizer.batch_decode` í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë§Œ ë³¼ ë•ŒëŠ” `tokenizer.decode`ë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ ëª‡ ìœ„ì¸ê°€? [SEP] ì„œìš¸ì€ êµ­ì œì ì¸ ë„ì‹œì´ë‹¤. ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ 4ìœ„ì´ë‹¤. [SEP]',\n",
       " '[CLS] ë„·í”Œë¦­ìŠ¤ëŠ” ëª‡ ê°œêµ­ì—ì„œ ì´ìš© ì¤‘ì¸ê°€? [SEP] ë„·í”Œë¦­ìŠ¤ëŠ” ì „ ì„¸ê³„ 190ê°œêµ­ ì´ìƒì—ì„œ ì´ìš© ì¤‘ì´ë‹¤. ìŠ¤ë¬¼ë‹¤ì„¯ìŠ¤ë¬¼í•˜ë‚˜ [SEP]',\n",
       " '[CLS] ë„·í”Œë¦­ìŠ¤ëŠ” ëª‡ ê°œêµ­ì—ì„œ ì´ìš© ì¤‘ì¸ê°€? [SEP] ì„¸ê³„ 190ê°œêµ­ ì´ìƒì—ì„œ ì´ìš© ì¤‘ì´ë‹¤. ìŠ¤ë¬¼ë‹¤ì„¯ìŠ¤ë¬¼í•˜ë‚˜ ì¬ë°Œë‹¤. [SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenized_examples.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ ëª‡ ìœ„ì¸ê°€? [SEP] ì„œìš¸ì€ êµ­ì œì ì¸ ë„ì‹œì´ë‹¤. ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ 4ìœ„ì´ë‹¤. [SEP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(te1.ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì•„ê¹Œ `stride=16`ìœ¼ë¡œ ì œê³µí–ˆëŠ”ë° ì˜ ë¶„ì ˆë˜ì—ˆëŠ”ì§€ ë³¼ê¹Œìš”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3665, 7175, 2019, 2226, 3658, 27135, 3774, 1570, 28674, 18, 10514, 2062, 2839, 2255, 2266, 8705]\n",
      "[3665, 7175, 2019, 2226, 3658, 27135, 3774, 1570, 28674, 18, 10514, 2062, 2839, 2255, 2266, 8705]\n"
     ]
    }
   ],
   "source": [
    "sep_index = te3.ids.index(tokenizer.sep_token_id) + 1\n",
    "\n",
    "print(te2.ids[-(doc_stride + 1):-1])\n",
    "print(te3.ids[sep_index:sep_index + doc_stride])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì˜ ë¶„ì ˆëœ ê²ƒ ê°™ë„¤ìš”. ë‚˜ë¨¸ì§€ 4ê°œì˜ featureë„ ì‚´í´ë´…ì‹œë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `token_type_ids`\n",
    "\n",
    "ë‘ ë¬¸ì¥ (ìš°ë¦¬ì˜ ê²½ìš° question & context)ë¥¼ ë„£ì„ ë•Œ ë‘ ë¬¸ì¥ì„ êµ¬ë¶„í•  ìˆ˜ ìˆë„ë¡ ì œê³µë˜ëŠ” ìë£Œí˜•ì…ë‹ˆë‹¤. ë§ë¡œ ë´ì„œëŠ” ë­”ì§€ ëª¨ë¥´ê² ìœ¼ë‹ˆ ë‹¤ì‹œ ë´…ì‹œë‹¤.\n",
    "`Encoding`ì—ì„œëŠ” `type_ids`ë¡œ í˜¸ì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 3671, 2079, 9476, 2259, 3665, 1077, 18431, 2116, 35, 3, 3671, 2073, 3854, 31221, 3763, 28674, 18, 3671, 2079, 9476, 2259, 3665, 24, 2090, 28674, 18, 3]\n"
     ]
    }
   ],
   "source": [
    "print(te1.type_ids)\n",
    "print(te1.ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹¤ì œë¡œ 0ê³¼ 1ì´ ê°ê° Questionê³¼ Context ìœ„ì¹˜ë¥¼ ë‹´ê³  ìˆëŠ”ì§€ ì‚´í´ë³¼ê¹Œìš”? (`target = te2`ë‚˜ `te3`ìœ¼ë¡œ ë°”ê¿”ë†“ê³ ë„ í•´ë³´ì„¸ìš”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`token_type_id` = 0\n",
      "Raw `input_ids` = [2, 3671, 2079, 9476, 2259, 3665, 1077, 18431, 2116, 35, 3]\n",
      "Decoded Result  = [CLS] ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ ëª‡ ìœ„ì¸ê°€? [SEP]\n",
      "\n",
      "`token_type_id` = 1\n",
      "Raw `input_ids` = [3671, 2073, 3854, 31221, 3763, 28674, 18, 3671, 2079, 9476, 2259, 3665, 24, 2090, 28674, 18, 3]\n",
      "Decoded Result  = ì„œìš¸ì€ êµ­ì œì ì¸ ë„ì‹œì´ë‹¤. ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ 4ìœ„ì´ë‹¤. [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = te1\n",
    "\n",
    "for token_type_id in np.unique(target.type_ids):\n",
    "    mask = target.type_ids == token_type_id\n",
    "    _ids = np.array(target.ids)[mask].tolist()\n",
    "    \n",
    "    print(f\"`token_type_id` = {token_type_id}\")\n",
    "    print(f\"Raw `input_ids` = {_ids}\")\n",
    "    print(f\"Decoded Result  = {tokenizer.decode(_ids)}\", end=\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `attention_mask`\n",
    "\n",
    "ì‚¬ì‹¤ ì´ê±´ ì‚´í´ë³¼ ê²ƒë„ ì—†ì´ ì „ë¶€ 1ì´ê¸´í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "for attn in tokenized_examples[\"attention_mask\"]:\n",
    "    print(attn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `offset_mapping`\n",
    "\n",
    "ì‚¬ì‹¤ ì œì¼ ì§ê´€ì ì´ì§€ ì•Šìœ¼ë©´ì„œ í•´ì„ì´ ë‚œí•´í•œ ë‘ ê°€ì§€ê°€ ë°”ë¡œ `offset_mapping`ê³¼ `overflow_to_sample_mapping`ì¸ë°ìš”, ëˆˆìœ¼ë¡œ ë³´ëŠ” ê²Œ ì´í•´ê°€ ì¡°ê¸ˆ ë” ì‰½ìŠµë‹ˆë‹¤.   \n",
    "`offset_mapping`ì€ ì˜¤ì•„ì—ì„œ ì„¤ëª…í–ˆë“¯ì´ ì‹¤ì œ í† í°ì´ **ì›ë˜ original textì—ì„œ ì–´ëŠ ìœ„ì¹˜ë¥¼ ê°€ì§€ëŠ”ì§€ í‘œì‹œ**í•©ë‹ˆë‹¤.\n",
    "\n",
    "![image](./assets/02_offset_mapping.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: ['ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ ëª‡ ìœ„ì¸ê°€?', 'ì„œìš¸ì€ êµ­ì œì ì¸ ë„ì‹œì´ë‹¤. ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ 4ìœ„ì´ë‹¤.']\n",
      "\n",
      "Token                     : [CLS]\n",
      "Offset                    : (0, 0)\n",
      "Text retreived with offset: \n",
      "\n",
      "Token                     : ì„œìš¸\n",
      "Offset                    : (0, 2)\n",
      "Text retreived with offset: ì„œìš¸\n",
      "\n",
      "Token                     : ##ì˜\n",
      "Offset                    : (2, 3)\n",
      "Text retreived with offset: ì˜\n",
      "\n",
      "Token                     : GDP\n",
      "Offset                    : (4, 7)\n",
      "Text retreived with offset: GDP\n",
      "\n",
      "Token                     : ##ëŠ”\n",
      "Offset                    : (7, 8)\n",
      "Text retreived with offset: ëŠ”\n",
      "\n",
      "Token                     : ì„¸ê³„\n",
      "Offset                    : (9, 11)\n",
      "Text retreived with offset: ì„¸ê³„\n",
      "\n",
      "Token                     : ëª‡\n",
      "Offset                    : (12, 13)\n",
      "Text retreived with offset: ëª‡\n",
      "\n",
      "Token                     : ìœ„ì¸\n",
      "Offset                    : (14, 16)\n",
      "Text retreived with offset: ìœ„ì¸\n",
      "\n",
      "Token                     : ##ê°€\n",
      "Offset                    : (16, 17)\n",
      "Text retreived with offset: ê°€\n",
      "\n",
      "Token                     : ?\n",
      "Offset                    : (17, 18)\n",
      "Text retreived with offset: ?\n",
      "\n",
      "Token                     : [SEP]\n",
      "Offset                    : (0, 0)\n",
      "Text retreived with offset: \n",
      "\n",
      "Token                     : ì„œìš¸\n",
      "Offset                    : (0, 2)\n",
      "Text retreived with offset: ì„œìš¸\n",
      "\n",
      "Token                     : ##ì€\n",
      "Offset                    : (2, 3)\n",
      "Text retreived with offset: ì€\n",
      "\n",
      "Token                     : êµ­ì œ\n",
      "Offset                    : (4, 6)\n",
      "Text retreived with offset: êµ­ì œ\n",
      "\n",
      "Token                     : ##ì ì¸\n",
      "Offset                    : (6, 8)\n",
      "Text retreived with offset: ì ì¸\n",
      "\n",
      "Token                     : ë„ì‹œ\n",
      "Offset                    : (9, 11)\n",
      "Text retreived with offset: ë„ì‹œ\n",
      "\n",
      "Token                     : ##ì´ë‹¤\n",
      "Offset                    : (11, 13)\n",
      "Text retreived with offset: ì´ë‹¤\n",
      "\n",
      "Token                     : .\n",
      "Offset                    : (13, 14)\n",
      "Text retreived with offset: .\n",
      "\n",
      "Token                     : ì„œìš¸\n",
      "Offset                    : (15, 17)\n",
      "Text retreived with offset: ì„œìš¸\n",
      "\n",
      "Token                     : ##ì˜\n",
      "Offset                    : (17, 18)\n",
      "Text retreived with offset: ì˜\n",
      "\n",
      "Token                     : GDP\n",
      "Offset                    : (19, 22)\n",
      "Text retreived with offset: GDP\n",
      "\n",
      "Token                     : ##ëŠ”\n",
      "Offset                    : (22, 23)\n",
      "Text retreived with offset: ëŠ”\n",
      "\n",
      "Token                     : ì„¸ê³„\n",
      "Offset                    : (24, 26)\n",
      "Text retreived with offset: ì„¸ê³„\n",
      "\n",
      "Token                     : 4\n",
      "Offset                    : (27, 28)\n",
      "Text retreived with offset: 4\n",
      "\n",
      "Token                     : ##ìœ„\n",
      "Offset                    : (28, 29)\n",
      "Text retreived with offset: ìœ„\n",
      "\n",
      "Token                     : ##ì´ë‹¤\n",
      "Offset                    : (29, 31)\n",
      "Text retreived with offset: ì´ë‹¤\n",
      "\n",
      "Token                     : .\n",
      "Offset                    : (31, 32)\n",
      "Text retreived with offset: .\n",
      "\n",
      "Token                     : [SEP]\n",
      "Offset                    : (0, 0)\n",
      "Text retreived with offset: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_index = 0\n",
    "target = tokenized_examples[target_index]\n",
    "\n",
    "orig_texts = [examples[\"question\"][target_index],\n",
    "              examples[\"context\"][target_index]]\n",
    "print(f\"Original Text: {orig_texts}\", end=\"\\n\\n\")\n",
    "for token, type_id, offset in zip(target.tokens, target.type_ids, target.offsets):\n",
    "    print(f\"Token                     : {token}\")\n",
    "    print(f\"Offset                    : {offset}\")\n",
    "\n",
    "    s, e = offset\n",
    "    print(f\"Text retreived with offset: {orig_texts[type_id][s:e]}\", end=\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëˆˆìœ¼ë¡œ ë³´ë‹ˆ ì œë²• ì§ê´€ì ì´ì£ ? ì´ê²Œ ì™œ í•„ìš”í•œì§€ëŠ” ëˆˆì¹˜ê°€ ë°±ë‹¨ì´ì‹ ë¶„ë“¤ì€ ëˆˆì¹˜ ì±˜ê² ì§€ë§Œ, `answer`ì˜ í† í°ìœ„ì¹˜ë¥¼ ë½‘ì•„ì˜¤ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. í˜„ì¬ ì›ë³¸ ë°ì´í„°ì˜ `answer` ë‚´ì—ëŠ” answerì˜ ì›ë³¸í…ìŠ¤íŠ¸ ë‚´ì—ì„œì˜ ìœ„ì¹˜ë§Œ ê°€ì§€ê³  ìˆì–´ìš”. ì–˜ë¥¼ í† í°í™”ëœ ê²°ê³¼ì˜ ìœ„ì¹˜ë¥¼ ë½‘ì•„ì£¼ë ¤ë©´ `offset_mapping`ì´ í•„ìš”í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `overflow_to_sample_mapping`\n",
    "\n",
    "`tokenizer`ì˜ ë§ˆì§€ë§‰ ê²°ê³¼ì¸ `overflow_to_sample_mapping`ì„ ì‚´í´ë´…ì‹œë‹¤. ì˜¤ì•„ì—ì„œ ì–¸ê¸‰í–ˆë“¯, í•´ë‹¹ featureëŠ” ì‹¤ì œë¡œ ì£¼ì–´ì§„ ì…ë ¥ ë°ì´í„°ê°€ ê¸¸ì´ê°€ ê¸¸ì–´ì ¸ ìª¼ê°œì§„ ì˜ˆì œ2ë²ˆê³¼ ê°™ì€ ë¬¸ì¥ì´ ìˆì„ ë•Œ ì›ë³¸ë°ì´í„°ì—ì„œ ëª‡ ë²ˆì§¸ ë°ì´í„°ì¸ì§€ ì•Œë ¤ì£¼ëŠ” ì •ë³´ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ê²Œ ì™œ í•„ìš”í• ê¹Œìš”? ìš°ë¦¬ì˜ ì˜ˆì œì—ëŠ” question, contextë§Œ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼, answerë„ ìˆëŠ”ë° ìš°ë¦¬ê°€ answerëŠ” `tokenizer`ì— ë”°ë¡œ íƒœì›Œì£¼ì§€ ì•Šê¸° ë•Œë¬¸ì— ì–˜ëŠ” ì–´ë¦¬ë‘¥ì ˆí•˜ê²Œ ìê¸°ë‘ í•œìŒì´ì—ˆë˜ question/contextê°€ ìª¼ê°œì§„ì§€ ëª¨ë¥´ê³  ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. í˜„ì¬ ìš°ë¦¬ì˜ extraction-based MRCì—ì„œëŠ” ì •ë‹µ í…ìŠ¤íŠ¸ê°€ ê·¸ëŒ€ë¡œ í•„ìš”í•˜ê¸°ë³´ë‹¤ëŠ”, í† í°í™”ëœ ê²°ê³¼ì—ì„œ ì •ë‹µì´ \"ëª‡ ë²ˆì§¸ë¶€í„° ëª‡ ë²ˆì¨°\"ì— ìœ„ì¹˜í•˜ëŠ”ì§€ê°€ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_examples[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í°í™”ëœ ê²°ê³¼ ë‚´ì—ì„œì˜ index        : 0, [CLS] ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ ëª‡ ìœ„ì¸ê°€? [SEP] ì„œìš¸ì€ êµ­ì œì ì¸ ë„ì‹œì´ë‹¤. ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ 4ìœ„ì´ë‹¤. [SEP]\n",
      "í•´ë‹¹ ë°ì´í„°ì˜ ì›ë³¸ë°ì´í„° ë‚´ì—ì„œì˜ index: 0, ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ ëª‡ ìœ„ì¸ê°€? ì„œìš¸ì€ êµ­ì œì ì¸ ë„ì‹œì´ë‹¤. ì„œìš¸ì˜ GDPëŠ” ì„¸ê³„ 4ìœ„ì´ë‹¤.\n",
      "\n",
      "í† í°í™”ëœ ê²°ê³¼ ë‚´ì—ì„œì˜ index        : 1, [CLS] ë„·í”Œë¦­ìŠ¤ëŠ” ëª‡ ê°œêµ­ì—ì„œ ì´ìš© ì¤‘ì¸ê°€? [SEP] ë„·í”Œë¦­ìŠ¤ëŠ” ì „ ì„¸ê³„ 190ê°œêµ­ ì´ìƒì—ì„œ ì´ìš© ì¤‘ì´ë‹¤. ìŠ¤ë¬¼ë‹¤ì„¯ìŠ¤ë¬¼í•˜ë‚˜ [SEP]\n",
      "í•´ë‹¹ ë°ì´í„°ì˜ ì›ë³¸ë°ì´í„° ë‚´ì—ì„œì˜ index: 1, ë„·í”Œë¦­ìŠ¤ëŠ” ëª‡ ê°œêµ­ì—ì„œ ì´ìš© ì¤‘ì¸ê°€? ë„·í”Œë¦­ìŠ¤ëŠ” ì „ ì„¸ê³„ 190ê°œêµ­ ì´ìƒì—ì„œ ì´ìš© ì¤‘ì´ë‹¤. ìŠ¤ë¬¼ë‹¤ì„¯ìŠ¤ë¬¼í•˜ë‚˜ ì¬ë°Œë‹¤.\n",
      "\n",
      "í† í°í™”ëœ ê²°ê³¼ ë‚´ì—ì„œì˜ index        : 2, [CLS] ë„·í”Œë¦­ìŠ¤ëŠ” ëª‡ ê°œêµ­ì—ì„œ ì´ìš© ì¤‘ì¸ê°€? [SEP] ì„¸ê³„ 190ê°œêµ­ ì´ìƒì—ì„œ ì´ìš© ì¤‘ì´ë‹¤. ìŠ¤ë¬¼ë‹¤ì„¯ìŠ¤ë¬¼í•˜ë‚˜ ì¬ë°Œë‹¤. [SEP]\n",
      "í•´ë‹¹ ë°ì´í„°ì˜ ì›ë³¸ë°ì´í„° ë‚´ì—ì„œì˜ index: 1, ë„·í”Œë¦­ìŠ¤ëŠ” ëª‡ ê°œêµ­ì—ì„œ ì´ìš© ì¤‘ì¸ê°€? ë„·í”Œë¦­ìŠ¤ëŠ” ì „ ì„¸ê³„ 190ê°œêµ­ ì´ìƒì—ì„œ ì´ìš© ì¤‘ì´ë‹¤. ìŠ¤ë¬¼ë‹¤ì„¯ìŠ¤ë¬¼í•˜ë‚˜ ì¬ë°Œë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, ovf in enumerate(tokenized_examples[\"overflow_to_sample_mapping\"]):\n",
    "    _tkd = tokenizer.decode(tokenized_examples[\"input_ids\"][idx])\n",
    "    print(f\"í† í°í™”ëœ ê²°ê³¼ ë‚´ì—ì„œì˜ index        : {idx}, {_tkd}\")\n",
    "    print(f\"í•´ë‹¹ ë°ì´í„°ì˜ ì›ë³¸ë°ì´í„° ë‚´ì—ì„œì˜ index: {ovf}, {examples['question'][ovf]} {examples['context'][ovf]}\", end=\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œì•¼ tokenizerì˜ ê²°ê³¼ê°€ ë‹¤ ì´í•´ë˜ëŠ” ê²ƒ ê°™ë„¤ìš”. ì—¬ê¸°ê¹Œì§€ë„ ì˜¤ëœ ì‹œê°„ì´ ê±¸ë ¸ëŠ”ë° ì‚¬ì‹¤ ì—¬ê¸°ê°€ ëì´ ì•„ë‹Œê±° ì•„ì‹œì£ ...? ì•„ì§ ëª¨ë¸ì—ê²Œ ì œê³µí•  ìˆ˜ëŠ” ì—†ëŠ” ìƒíƒœì˜ ë°ì´í„°ì…ë‹ˆë‹¤. ë² ì´ìŠ¤ë¼ì¸ì—ì„œ `prepare_train_features`ì˜ í•œ ë‹¨ê³„ ë°–ì— ì˜¤ì§€ ì•Šì•˜ì–´ìš”. í† í°í™”ëœ ê²°ê³¼ë¥¼ ì–´ë–»ê²Œ QA Taskì— ë§ì¶°ì„œ ë³€ê²½í•˜ëŠ”ì§€ ì‚´í´ë´…ì‹œë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Preprocess to QA Task\n",
    "\n",
    "ìš°ì„  ì½”ë“œë¥¼ ì‚´í´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train preprocessing / ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "def prepare_train_features(examples):\n",
    "    ###############################\n",
    "    ####### ì§€ê¸ˆ ì—¬ê¸°ê¹Œì§€ ë³¸ê±°ì„ #######\n",
    "    ###############################\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        # return_token_type_ids=False, # robertaëª¨ë¸ì„ ì‚¬ìš©í•  ê²½ìš° False, bertë¥¼ ì‚¬ìš©í•  ê²½ìš° Trueë¡œ í‘œê¸°í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "        padding=\"max_length\" if pad_to_max_length else False,\n",
    "    )\n",
    "    ###############################\n",
    "    ###############################\n",
    "    ###############################\n",
    "\n",
    "\n",
    "    # ì—¬ê¸°ë¶€í„° ë´…ì‹œë‹¤. ì½”ë“œë§Œ ë³´ë ¤ê³  ì—¬ê¸°ì„  ì£¼ì„ì„ ì œê±°í–ˆì–´ìš”\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[answer_column_name][sample_index]\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if not (\n",
    "                offsets[token_start_index][0] <= start_char\n",
    "                and offsets[token_end_index][1] >= end_char\n",
    "            ):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                while (\n",
    "                    token_start_index < len(offsets)\n",
    "                    and offsets[token_start_index][0] <= start_char\n",
    "                ):\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë“œë””ì–´ `sample_mapping`ê³¼ `offset_mapping`ì´ ë¬´ì—‡ì„ í•˜ëŠ”ì§€ ì´í•´í–ˆë„¤ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "tokenized_examples[\"start_positions\"] = []\n",
    "tokenized_examples[\"end_positions\"] = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë£¨í”„ë¥¼ `offset_mapping`ì— ëŒ€í•´ ëŒë ¤ì£¼ë„¤ìš”. ì•„ê¹Œ ì–¸ê¸‰í–ˆë“¯ì´ ì •ë‹µë°ì´í„°ë¥¼ ê°™ì´ í™œìš©í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì•„ë˜ ì½”ë“œëŠ” ì˜ë„ì ìœ¼ë¡œ `breakë¥¼` ê±¸ì–´ë‘ì—ˆëŠ”ë°ìš”, ì¤‘ê°„ ë³€ìˆ˜ë¥¼ ì‚´í´ë³´ì‹œë¼ê³  ê±¸ì–´ë‘ì—ˆìŠµë‹ˆë‹¤. ì¼ë‹¨ì€ 0ë²ˆ ë°ì´í„°ì— ëŒ€í•´ì„œ ê±¸ì–´ë‘ì—ˆì–´ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "meomchow = 0\n",
    "for i, offsets in enumerate(offset_mapping):\n",
    "    input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "    cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "    sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "    sample_index = sample_mapping[i]\n",
    "    answers = examples[answer_column_name][sample_index]\n",
    "\n",
    "    if len(answers[\"answer_start\"]) == 0:\n",
    "        tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "        tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "    else:\n",
    "        start_char = answers[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "            token_start_index += 1\n",
    "\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "            token_end_index -= 1\n",
    "\n",
    "        if not (\n",
    "            offsets[token_start_index][0] <= start_char\n",
    "            and offsets[token_end_index][1] >= end_char\n",
    "        ):\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            while (\n",
    "                token_start_index < len(offsets)\n",
    "                and offsets[token_start_index][0] <= start_char\n",
    "            ):\n",
    "                token_start_index += 1\n",
    "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "            while offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    if i == meomchow:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ì—ì„œë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ì‚´í´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_examples[\"start_positions\"] = []\n",
    "tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "sample_index = sample_mapping[i]\n",
    "answers = examples[answer_column_name][sample_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_start': [24], 'text': ['ì„¸ê³„ 4ìœ„']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤í–‰íˆ ì—¬ê¸° ìˆëŠ” ë³€ìˆ˜ëŠ” ëª¨ë‘ ì•„ëŠ” ë³€ìˆ˜ë„¤ìš”. ë³µê¸°í•˜ë©´\n",
    "- `input_ids`: í† í°í™”ëœ ê²°ê³¼\n",
    "- `cls_index`: `cls` í† í°ì˜ ìœ„ì¹˜ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” 0ë²ˆ ë°–ì— ë‚˜ì˜¬ ê²Œ ì—†ì–´ìš”\n",
    "- `sequence_ids`: ì•„ê¹Œ ì‚´í´ë³¸ `token_type_ids`ì™€ ì •í™•íˆ ê°™ì€ ê¸°ëŠ¥ì¸ë° ì°¨ì´ì ì€ ë”± í•˜ë‚˜: special_token([CLS], [SEP]) ìë¦¬ì— 0, 1ì´ ì•„ë‹Œ `None`ì„ ë°€ì–´ë„£ì–´ì¤ë‹ˆë‹¤.\n",
    "- `sample_mapping`: ì›ë³¸ í…ìŠ¤íŠ¸ ë‚´ì—ì„œì˜ í† í°ì˜ ìœ„ì¹˜ì…ë‹ˆë‹¤\n",
    "- `answers`: ì›ë³¸ ì…ë ¥ì—ì„œ `answer` ë½‘ì•„ì™”ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(answers[\"answer_start\"]) == 0:\n",
    "    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "    tokenized_examples[\"end_positions\"].append(cls_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì²« ë²ˆì§¸ ë¶„ê¸°ì¸ë°ìš”, ì¡°ê±´ì´ `len(answers[\"answer_start\"]) == 0`ì´ë„¤ìš”. ì •ë‹µì´ ì—†ëŠ” ê²½ìš°ëŠ” ê·¸ëƒ¥ `cls_index`ë¥¼ ì •ë‹µìœ¼ë¡œ ë°€ì–´ë„£ë„ë¡ ì„¤ê³„ë˜ì–´ìˆë„¤ìš”. edge caseì˜ ê²½ìš°ë‹ˆê¹Œ elseë¬¸ìœ¼ë¡œ ë„˜ì–´ê°‘ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 29\n",
      "Extracted answer : ì„¸ê³„ 4ìœ„\n",
      "Actual answer    : ì„¸ê³„ 4ìœ„\n"
     ]
    }
   ],
   "source": [
    "# ë§ˆìŒ ì†ì— `else`ê°€ ì—¬ê¸° ìˆì—ˆë‹¤ê³  ìƒê°í•˜ì„¸ìš”\n",
    "\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "print(start_char, end_char)\n",
    "print(f\"Extracted answer : {examples['context'][meomchow][start_char:end_char]}\")\n",
    "print(f\"Actual answer    : {examples['answers'][meomchow]['text'][0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì›ë³¸ í…ìŠ¤íŠ¸ ë‚´ì—ì„œì˜ ì •ë‹µ ìœ„ì¹˜ë¥¼ ë½‘ì•„ì™”ìŠµë‹ˆë‹¤. 24ë²ˆì§¸ë¶€í„° 29ë²ˆì§¸ê¹Œì§€ê°€ ì •ë‹µ ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” ìœ„ì¹˜ì…ë‹ˆë‹¤. end_charëŠ” ë‹¨ìˆœíˆ í…ìŠ¤íŠ¸ ê¸¸ì´ë¡œ ë½‘ì•„ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(pad_on_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "    token_start_index += 1\n",
    "\n",
    "token_end_index = len(input_ids) - 1\n",
    "while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "    token_end_index -= 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•´ë‹¹ ë¶€ë¶„ì€ **ì „ì²´ í† í° ê²°ê³¼ ì¤‘ì— contextì˜ ì‹œì‘ì ê³¼ ëì **ì„ ì°¾ì•„ì¤ë‹ˆë‹¤. ì—¬ê¸°ì„œ ìì£¼ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ëŠ” ë¶€ë¶„ì´ `pad_on_right`ì¸ë°ìš”, ì–˜ëŠ” contextë¥¼ ì˜¤ë¥¸ìª½ì—ë‹¤ ë°€ì–´ë„£ì„ ê²ƒì¸ì§€ ì™¼ìª½ì— ë°€ì–´ë„£ì„ ê²ƒì¸ì§€ ì…ë‹ˆë‹¤. ì½”ë“œë¥¼ ë°”ê¿” ì“°ë©´\n",
    "```python\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "token_end_index = len(input_ids) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`sequence_ids` = [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n",
      "Extracted context [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"`sequence_ids` = {sequence_ids}\")\n",
    "\n",
    "print(f\"Extracted context {sequence_ids[token_start_index:token_end_index]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contextë§Œ ì˜ ë½‘íŒ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (\n",
    "    offsets[token_start_index][0] <= start_char\n",
    "    and offsets[token_end_index][1] >= end_char\n",
    "):\n",
    "    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "    tokenized_examples[\"end_positions\"].append(cls_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì²« ë²ˆì§¸ ë¶„ê¸°ë¶€í„° ì‚´í´ë´…ì‹œë‹¤. `offsets`ëŠ” ê³„ì† ì–¸ê¸‰í–ˆì§€ë§Œ ì›ë³¸ í…ìŠ¤íŠ¸ ë‚´ì—ì„œì˜ í† í°ì´ ê°€ì§€ëŠ” ìœ„ì¹˜ì…ë‹ˆë‹¤.\n",
    "![image](./assets/02_offset_mapping.png)\n",
    "\n",
    "ifë¬¸ ë‚´ì—ì„œ ë¹„êµë˜ëŠ” ë°ì´í„°ë“¤ì€ ëª¨ë‘ **ì›ë³¸ ë°ì´í„° ë‚´ì—ì„œì˜ ìœ„ì¹˜ë¥¼ ë¹„êµ**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "0\n",
      "24\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# token_start_index = contextê°€ ì‹œì‘í•˜ëŠ” tokení™” ê²°ê³¼ ë‚´ì˜ ìœ„ì¹˜\n",
    "print(token_start_index)\n",
    "\n",
    "# offsets[token_start_index] = contextê°€ ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ì‹œì‘í•˜ëŠ” ìœ„ì¹˜.\n",
    "# ë‹¹ì—°íˆ contextê°€ ì‹œì‘í•˜ëŠ” ìœ„ì¹˜ë‹ˆê¹Œ 0ì´ ë‚˜ì™€ì•¼ ì •ìƒì…ë‹ˆë‹¤.\n",
    "print(offsets[token_start_index][0])\n",
    "\n",
    "# ì´ ê°’ì„ start_charì™€ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "print(start_char)\n",
    "print(offsets[token_start_index][0] <= start_char)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§Œì•½ ì •ë‹µì´ context ë‚´ì— ì˜ ìœ„ì¹˜í•´ìˆë‹¤ê³  ìƒê°í•´ë´…ì‹œë‹¤.   \n",
    "ì •ë‹µì˜ ì‹œì‘ì€ ë‹¹ì—°íˆ contextì˜ ì‹œì‘ë³´ë‹¤ ë’¤ì— ë‚˜ì™€ì•¼í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. end_charë¥¼ ë¹„êµí•˜ëŠ” ê²ƒë„ ê°™ì€ ë§¥ë½ì…ë‹ˆë‹¤. ì‹¤ì œë¡œ contextê°€ ëë‚˜ëŠ” ìœ„ì¹˜ë³´ë‹¤ í•­ìƒ end_charê°€ ì‘ë„ë¡ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "ì—¬ê¸°ì— `not`ì„ ë¶™ì—¬ì£¼ë‹ˆê¹Œ **ì •ë‹µì´ context ë‚´ì— ì˜ ìœ„ì¹˜í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°**ë¥¼ ì¡ì•„ì£¼ëŠ” ê²ƒì´ë„¤ìš”! ì—¬ê¸°ë„ edge caseë¥¼ ì¡ì•„ì£¼ëŠ” ê²ƒì´ë¼ ë³¼ ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê²½ìš°ì—ë„ ë§ˆì°¬ê°€ì§€ë¡œ `cls_index`ë¥¼ ì •ë‹µìœ¼ë¡œ ì˜ˆì¸¡í•˜ë„ë¡ ë°€ì–´ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§Œì•½ ì •ë‹µì´ ì˜ context ë‚´ì— ë“¤ì–´ìˆë‹¤ë©´ ifë¥¼ í†µê³¼í•˜ê³  elseë¡œ ë„˜ì–´ì˜¤ê² ë„¤ìš”. ê·¸ ì½”ë“œê°€ ì•„ë˜ì— í•´ë‹¹í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë²ˆì—” ë§ˆìŒ ì†ì˜ if ê°€ ìˆì—ˆë‹¤ê³  ì¹©ì‹œë‹¤.\n",
    "while (\n",
    "    token_start_index < len(offsets)\n",
    "    and offsets[token_start_index][0] <= start_char\n",
    "):\n",
    "    token_start_index += 1\n",
    "tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "\n",
    "while offsets[token_end_index][1] >= end_char:\n",
    "    token_end_index -= 1\n",
    "tokenized_examples[\"end_positions\"].append(token_end_index + 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token_start_indexëŠ” contextì˜ ì‹œì‘ì ì´ê³  len(offsets)ëŠ” ì „ì²´ token ê°œìˆ˜ì— í•´ë‹¹í•©ë‹ˆë‹¤.\n",
    "`offsets[token_start_index[0]] <= start_char`ëŠ” ì•„ê¹Œë„ í™•ì¸í–ˆë“¯ì´ ì •ë‹µì˜ ì‹œì‘ ìœ„ì¹˜ê°€ contextë³´ë‹¤ ë’¤ì— ì •ìƒì ìœ¼ë¡œ ì•ˆì°©í•œ ê²½ìš°ë¥¼ ëœ»í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì—¬ê¸°ì„œ token_start_indexë¥¼ ê³„ì† `+= 1`ì”© ì¦ê°€ì‹œì¼œì£¼ëŠ”ë°ìš”, ì´ì œë¶€í„°ëŠ” `token_start_index`ê°€ contextì˜ ì‹œì‘ìœ„ì¹˜ê°€ ì•„ë‹Œ ì •ë‹µì˜ í† í°ì‹œì‘ ìœ„ì¹˜ë¥¼ ì•Œë¦¬ëŠ” ë³€ìˆ˜ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "ì´ loopê°€ ì–¸ì œ ê¹¨ì§ˆê¹Œìš”? `token_start_index`ëŠ” ê³„ì† 1ì”© ì¦ê°€í• í…Œë‹ˆ\n",
    "1. ì‹¤ì œ ì •ë‹µ ìœ„ì¹˜ë³´ë‹¤ ì•ì„œë‚˜ê°ˆ ë•Œ\n",
    "2. í˜¹ì€ ì •ë‹µì„ ëª» ì°¾ì•„ì„œ ê²°êµ­ ì „ì²´ token ê¸¸ì´ë³´ë‹¤ ì»¤ì ¸ë²„ë¦´ ë•Œ\n",
    "\n",
    "ìœ„ì˜ ê²½ìš°ëŠ” ì •ë‹µì„ ì°¾ì•„ë²„ë ¸ìœ¼ë‹ˆ +1í•˜ëŠ” ê³¼ì •ì—ì„œ ì •ë‹µ ë’¤ë¡œ ë„˜ì–´ê°„ ê²½ìš°ê² ì£ ? ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ `start_positions` ìœ„ì¹˜ì—ëŠ” -1ì„ í•´ì¤€ ê°’ì„ ë„£ì–´ì£¼ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ì˜ `end_position` ì°¾ëŠ” ê²ƒë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checkë¥¼ í•´ë´…ì‹œë‹¤. í•´ë‹¹ token positionì´ ì‹¤ì œë¡œ ì •ë‹µì„ ì˜ ë°”ë¼ë³´ê³  ìˆëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "st, et = tokenized_examples[\"start_positions\"][meomchow], tokenized_examples[\"end_positions\"][meomchow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì„¸ê³„ 4ìœ„'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_token = tokenized_examples.input_ids[meomchow][st:et + 1]\n",
    "tokenizer.decode(answer_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "íœ´ ë‹¤í–‰íˆ ê²°ê³¼ê°€ ì˜ ë‚˜ì˜¤ëŠ”êµ°ìš”. `meomchow` ë³€ìˆ˜ë¥¼ 1ì´ë‚˜ 2ë¡œ ë°”ê¿”ì„œ ë˜ test í•´ë³´ì„¸ìš”! ë§ˆì§€ë§‰ìœ¼ë¡œ ì‹¤ì œ `datasets.map`ì„ ê±¸ê³ ë‚œ ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. `num_rows`ì™€ `features` ì •ë³´ê°€ ì¼ë¶€ ë°”ë€ ê²ƒì„ í™•ì¸í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Preprocess\n",
      "Dataset({\n",
      "    features: ['context', 'question', 'answers', 'title', 'id', 'document_id'],\n",
      "    num_rows: 2\n",
      "})\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e354330799458688e88c9cd190dbd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Preprocess\n",
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_dict(examples)\n",
    "print(\"Before Preprocess\")\n",
    "print(train_dataset)\n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "print(\"After Preprocess\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `prepare_validation_features`\n",
    "\n",
    "ì˜¤ì•„ì—ì„œ ì‹œê°„ì´ ë¶€ì¡±í•´ ë‹¤ë£¨ì§€ ì•Šì€ ë¶€ë¶„ì¸ë°ìš”, ê°„ë‹¨í•˜ê²Œ ì‚´í´ë³´ëŠ” ì½”ë“œë¥¼ ë„£ì–´ë³´ì•˜ìŠµë‹ˆë‹¤. Validationì˜ ê²½ìš° start, end token ìœ„ì¹˜ë¥¼ ë§ì¶œ í•„ìš”ê°€ ì—†ì´ ì •ë‹µ textë§Œ ì˜ ê°€ì ¸ë‹¤ì£¼ë©´ ë˜ë‹¤ë³´ë‹ˆ ì •ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ì¼ì€ ë”°ë¡œ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹ ì— `tokenizer`ê°€ `max_seq_length` ë¡œì§ìœ¼ë¡œ **ì˜ë¼ ë²„ë¦° ë°ì´í„°ë“¤ì— ëŒ€í•´ ì •ë‹µë°ì´í„°ë¥¼ ë§¤ì¹­ì‹œì¼œì£¼ê¸° ìœ„í•´ `example_id`ë§Œ ì˜ ë„£ì–´ì¤ë‹ˆë‹¤.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_examples = tokenizer(\n",
    "    examples[question_column_name if pad_on_right else context_column_name],\n",
    "    examples[context_column_name if pad_on_right else question_column_name],\n",
    "    truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "    max_length=max_seq_length,\n",
    "    stride=doc_stride,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    # return_token_type_ids=False, # robertaëª¨ë¸ì„ ì‚¬ìš©í•  ê²½ìš° False, bertë¥¼ ì‚¬ìš©í•  ê²½ìš° Trueë¡œ í‘œê¸°í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "    padding=\"max_length\" if pad_to_max_length else False,\n",
    ")\n",
    "\n",
    "# ê¸¸ì´ê°€ ê¸´ contextê°€ ë“±ì¥í•  ê²½ìš° truncateë¥¼ ì§„í–‰í•´ì•¼í•˜ë¯€ë¡œ, í•´ë‹¹ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ mapping ê°€ëŠ¥í•œ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "# evaluationì„ ìœ„í•´, predictionì„ contextì˜ substringìœ¼ë¡œ ë³€í™˜í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "# corresponding example_idë¥¼ ìœ ì§€í•˜ê³  offset mappingsì„ ì €ì¥í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "    # sequence idë¥¼ ì„¤ì •í•©ë‹ˆë‹¤ (to know what is the context and what is the question).\n",
    "    sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "    context_index = 1 if pad_on_right else 0\n",
    "\n",
    "    # í•˜ë‚˜ì˜ exampleì´ ì—¬ëŸ¬ê°œì˜ spanì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    sample_index = sample_mapping[i]\n",
    "    tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "    # Set to None the offset_mappingì„ Noneìœ¼ë¡œ ì„¤ì •í•´ì„œ token positionì´ contextì˜ ì¼ë¶€ì¸ì§€ ì‰½ê²Œ íŒë³„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    tokenized_examples[\"offset_mapping\"][i] = [\n",
    "        (o if sequence_ids[k] == context_index else None)\n",
    "        for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ex-1', 'ex-2', 'ex-2']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_examples[\"example_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " (0, 2),\n",
       " (2, 3),\n",
       " (4, 6),\n",
       " (6, 8),\n",
       " (9, 11),\n",
       " (11, 13),\n",
       " (13, 14),\n",
       " (15, 17),\n",
       " (17, 18),\n",
       " (19, 22),\n",
       " (22, 23),\n",
       " (24, 26),\n",
       " (27, 28),\n",
       " (28, 29),\n",
       " (29, 31),\n",
       " (31, 32),\n",
       " None]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_examples[\"offset_mapping\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•¨ìˆ˜í™”í•´ì„œ ì‹¤ì œ ë³€í˜•ë˜ëŠ” ì¼€ì´ìŠ¤ë¥¼ ì‚´í´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        # return_token_type_ids=False, # robertaëª¨ë¸ì„ ì‚¬ìš©í•  ê²½ìš° False, bertë¥¼ ì‚¬ìš©í•  ê²½ìš° Trueë¡œ í‘œê¸°í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "        padding=\"max_length\" if pad_to_max_length else False,\n",
    "    )\n",
    "\n",
    "    # ê¸¸ì´ê°€ ê¸´ contextê°€ ë“±ì¥í•  ê²½ìš° truncateë¥¼ ì§„í–‰í•´ì•¼í•˜ë¯€ë¡œ, í•´ë‹¹ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ mapping ê°€ëŠ¥í•œ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # evaluationì„ ìœ„í•´, predictionì„ contextì˜ substringìœ¼ë¡œ ë³€í™˜í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "    # corresponding example_idë¥¼ ìœ ì§€í•˜ê³  offset mappingsì„ ì €ì¥í•´ì•¼í•©ë‹ˆë‹¤.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # sequence idë¥¼ ì„¤ì •í•©ë‹ˆë‹¤ (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # í•˜ë‚˜ì˜ exampleì´ ì—¬ëŸ¬ê°œì˜ spanì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mappingì„ Noneìœ¼ë¡œ ì„¤ì •í•´ì„œ token positionì´ contextì˜ ì¼ë¶€ì¸ì§€ ì‰½ê²Œ íŒë³„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Before Preprocess ****\n",
      "Dataset({\n",
      "    features: ['context', 'question', 'answers', 'title', 'id', 'document_id'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705d076e898d4c6abe137d2a8adf7aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** After Preprocess ****\n",
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "eval_examples = Dataset.from_dict(examples)\n",
    "print(\"**** Before Preprocess ****\")\n",
    "print(eval_examples)\n",
    "eval_dataset = eval_examples.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=eval_examples.column_names,\n",
    ")\n",
    "print(\"**** After Preprocess ****\")\n",
    "print(eval_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `postprocess_qa_predictions`\n",
    "\n",
    "í›„ì²˜ë¦¬ ê²°ê³¼ë„ ìƒë‹¹íˆ ì¤‘ìš”í•œë° ë§¤ë²ˆ ì‹œê°„ì´ ë¶€ì¡±í•´ì„œ ì‚´í´ë³´ì§€ ëª»í•´ì„œ ì§§ê²Œ ë„£ì–´ë´¤ìŠµë‹ˆë‹¤. ê³„ì† ì œì¼ ì¤‘ìš”í•œ ë¶€ë¶„ì€ ìª¼ê°œì§„ ë°ì´í„°ë“¤ì„ ì–´ë–»ê²Œ í•©ì³ì˜¤ëŠëƒì— ìˆì–´ìš”. ë‘ ê°œë¡œ ë¶„ì ˆë˜ëŠ” ì˜ˆì œ2ë²ˆì— ì£¼ì˜í•´ì£¼ì„¸ìš”  \n",
    "â€¼ï¸ ì£¼ì˜: ì•„ë˜ ì½”ë“œëŠ” ì €í¬ê°€ ì œê³µí•œ ë² ì´ìŠ¤ë¼ì¸ì—ì„œ ì°¨ìš©í•œ ê²ƒì€ ë§ìœ¼ë‚˜ ìµœì†Œí•œì˜ ì„¸íŒ…ì„ í•˜ê¸° ìœ„í•´ ì¼ë¶€ ì½”ë“œê°€ ë³€í˜•ë˜ì–´ ìˆìŠµë‹ˆë‹¤. êµ‰ì¥íˆ ì‚¬ì†Œí•˜ê²Œ ì°¨ì´ê°€ ìˆê¸° ë•Œë¬¸ì— ì£¼ì˜í•˜ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForQuestionAnswering, EvalPrediction, TrainingArguments\n",
    "import evaluate\n",
    "\n",
    "from trainer_qa import QuestionAnsweringTrainer\n",
    "from utils_qa import postprocess_qa_predictions\n",
    "\n",
    "training_args = TrainingArguments(do_eval=True, output_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
    "\n",
    "def post_processing_function(examples, features, predictions, training_args):\n",
    "    # Post-processing: start logitsê³¼ end logitsì„ original contextì˜ ì •ë‹µê³¼ matchì‹œí‚µë‹ˆë‹¤.\n",
    "    predictions = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        max_answer_length=max_answer_length,\n",
    "        output_dir=None,\n",
    "    )\n",
    "    # Metricì„ êµ¬í•  ìˆ˜ ìˆë„ë¡ Formatì„ ë§ì¶°ì¤ë‹ˆë‹¤.\n",
    "    formatted_predictions = [\n",
    "        {\"id\": k, \"prediction_text\": v} for k, v in predictions.items()\n",
    "    ]\n",
    "    if training_args.do_predict:\n",
    "        return formatted_predictions\n",
    "\n",
    "    elif training_args.do_eval:\n",
    "        references = [\n",
    "            {\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]}\n",
    "            for ex in eval_examples\n",
    "        ]\n",
    "        return EvalPrediction(\n",
    "            predictions=formatted_predictions, label_ids=references\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path, config=config)\n",
    "\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    eval_examples=eval_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    # data_collator=data_collator,\n",
    "    post_process_function=post_processing_function,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì ìœ„ì—ê¹Œì§€ëŠ” ê°„ë‹¨í•œ ì„¸íŒ…ì´ì—ˆêµ¬ìš” ì•„ë˜ëŠ” `QuestionAnsweringTrainer.evaluate` ë©”ì†Œë“œë¥¼ ì¼ë¶€ ê°€ì ¸ì˜¨ ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì£¼ì•ˆì ì€ `output`ì— ì–´ë–¤ ê²Œ ì–´ë–»ê²Œ ë“¤ì–´ì˜¤ëŠ”ì§€ ë³´ëŠ” ê²ƒì´ì—ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daehyuncho/anaconda3/envs/mrc/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee87288edbf4e9691be734b50decb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "self = trainer\n",
    "\n",
    "eval_dataset = self.eval_dataset\n",
    "eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "eval_examples = self.eval_examples\n",
    "\n",
    "ignore_keys = None\n",
    "\n",
    "# ì¼ì‹œì ìœ¼ë¡œ metric computationë¥¼ ë¶ˆê°€ëŠ¥í•˜ê²Œ í•œ ìƒíƒœì´ë©°, í•´ë‹¹ ì½”ë“œì—ì„œëŠ” loop ë‚´ì—ì„œ metric ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "compute_metrics = self.compute_metrics\n",
    "self.compute_metrics = None\n",
    "try:\n",
    "    output = self.prediction_loop(\n",
    "        eval_dataloader,\n",
    "        description=\"Evaluation\",\n",
    "        # metricì´ ì—†ìœ¼ë©´ ì˜ˆì¸¡ê°’ì„ ëª¨ìœ¼ëŠ” ì´ìœ ê°€ ì—†ìœ¼ë¯€ë¡œ ì•„ë˜ì˜ ì½”ë“œë¥¼ ë”°ë¥´ê²Œ ë©ë‹ˆë‹¤.\n",
    "        # self.args.prediction_loss_only\n",
    "        prediction_loss_only=True if compute_metrics is None else None,\n",
    "        ignore_keys=ignore_keys,\n",
    "    )\n",
    "finally:\n",
    "    self.compute_metrics = compute_metrics\n",
    "    \n",
    "if isinstance(eval_dataset, Dataset):\n",
    "    eval_dataset.set_format(\n",
    "        type=eval_dataset.format[\"type\"],\n",
    "        columns=list(eval_dataset.features.keys()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.trainer_utils.EvalLoopOutput,\n",
       " EvalLoopOutput(predictions=(array([[-2.27704510e-01,  2.16861933e-01,  9.41800296e-01,\n",
       "          4.41306651e-01,  2.32540250e-01,  8.80979180e-01,\n",
       "         -4.31050867e-01, -1.55686215e-01,  6.72641158e-01,\n",
       "          3.40887159e-02,  1.96681008e-01,  3.32069993e-01,\n",
       "          5.18067956e-01,  1.04460359e+00,  4.56062078e-01,\n",
       "          2.61725456e-01,  5.75585783e-01,  2.71887213e-01,\n",
       "          1.98809683e-01,  8.62057626e-01,  3.42595547e-01,\n",
       "          5.80603421e-01,  7.32531667e-01,  7.42925227e-01,\n",
       "          1.18085706e+00,  6.21984661e-01,  2.02956215e-01,\n",
       "          1.96060240e-01, -1.35519832e-01, -4.08501625e-02,\n",
       "         -5.07392883e-01, -3.61381412e-01],\n",
       "        [-1.06058374e-01,  5.95110178e-01,  4.28310269e-03,\n",
       "         -5.18237948e-01, -5.51881194e-01,  3.06386590e-01,\n",
       "          1.50878072e-01,  5.29339790e-01,  3.29761326e-01,\n",
       "          7.90486276e-01,  5.05057216e-01,  1.51467636e-01,\n",
       "          5.33966959e-01,  1.93225041e-01,  8.24892163e-01,\n",
       "          5.75716496e-01,  5.72912395e-01,  6.03487007e-02,\n",
       "          7.51881301e-01, -1.65228739e-01,  3.83829176e-01,\n",
       "          1.86884701e-01,  4.72768694e-01,  8.84366810e-01,\n",
       "          1.59707949e-01,  2.00867847e-01, -2.98221279e-02,\n",
       "          1.35780990e-01, -2.21507251e-01, -1.78798765e-01,\n",
       "          2.04386264e-01,  1.50805160e-01],\n",
       "        [ 1.78931415e-01,  3.95215839e-01,  8.06451589e-02,\n",
       "         -4.91087884e-01, -4.12606657e-01,  3.32997739e-01,\n",
       "          4.59242612e-03,  5.02371192e-01,  2.73653567e-01,\n",
       "          7.88663208e-01,  5.49732864e-01,  8.03483427e-02,\n",
       "          8.67273271e-01,  5.62079966e-01,  3.39091234e-02,\n",
       "          6.59986794e-01, -1.26723751e-01,  2.87398010e-01,\n",
       "          1.69165522e-01,  4.74214673e-01,  7.43691862e-01,\n",
       "          2.62009919e-01, -9.22714476e-04,  1.34123117e-01,\n",
       "          1.50197044e-01, -1.13191590e-01, -7.84341916e-02,\n",
       "          2.53413528e-01, -5.73513269e-01,  2.49526709e-01,\n",
       "          7.13579953e-01,  8.00953582e-02]], dtype=float32), array([[ 0.06772858, -0.84322923,  0.88891345, -1.6484071 , -0.34553805,\n",
       "         -0.70521986,  0.25301734, -0.56876427, -1.1125365 ,  0.23267925,\n",
       "         -0.2838908 , -0.35224155,  0.93925375,  0.01903042,  0.6487261 ,\n",
       "         -0.23201995,  0.12373082, -0.26906085, -0.4524621 ,  0.9761932 ,\n",
       "         -1.3817466 , -0.03656138, -0.7128789 ,  0.30850235,  0.16006583,\n",
       "         -0.00567151, -0.29696274, -0.2852122 , -0.16919954, -0.23460476,\n",
       "         -0.02225418, -0.26817498],\n",
       "        [-0.02613476, -0.9810475 , -0.40576497, -0.14037646, -0.71665114,\n",
       "         -0.06561633, -0.49675557, -0.19822977, -0.01292498, -1.0211791 ,\n",
       "          0.09474035, -0.7746159 , -0.7124997 , -0.38906297, -0.74657756,\n",
       "         -1.1999291 , -0.27648658,  0.15342425, -0.8387967 , -0.16387095,\n",
       "          0.33270875, -0.43957824, -0.2138849 , -0.35450196, -0.77310985,\n",
       "         -0.31554428, -0.56158316, -0.39647993, -0.4877595 , -0.654433  ,\n",
       "         -0.09969018, -0.7577406 ],\n",
       "        [ 0.19663768, -0.8468073 , -0.31220955, -0.17815225, -0.67327845,\n",
       "         -0.08988103, -0.5915096 , -0.07594179,  0.00892548, -0.96642876,\n",
       "          0.29090872, -0.6740272 , -0.9213586 , -0.16951908,  0.18171574,\n",
       "         -0.9074101 , -0.0469621 ,  0.39108208, -0.58496135, -0.14376692,\n",
       "         -0.33773726, -0.62375486, -0.4502099 , -0.7379956 , -0.53366184,\n",
       "         -0.77002484, -0.69844913, -0.03068519,  0.43519562, -0.24978435,\n",
       "         -0.21473597, -0.6751834 ]], dtype=float32)), label_ids=None, metrics={}, num_samples=3))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output), output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EvalLoopOutput`ì€ ì™œì¸ì§€ ëª¨ë¥´ê² ëŠ”ë° `__dict__`ê°€ ì—†ì–´ì„œ ëˆˆìœ¼ë¡œ ë³´ê³  ê°€ì ¸ì˜¤ê±°ë‚˜ `dir`í•¨ìˆ˜ë¡œ ì§€ì›í•˜ëŠ” attributeë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count', 'index', 'label_ids', 'metrics', 'num_samples', 'predictions']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dir(output) if not i.startswith('_')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¤‘ìš”í•œê±´ `predictions`ë‹ˆê¹Œ `predictions`ë§Œ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple,\n",
       " (array([[-2.27704510e-01,  2.16861933e-01,  9.41800296e-01,\n",
       "           4.41306651e-01,  2.32540250e-01,  8.80979180e-01,\n",
       "          -4.31050867e-01, -1.55686215e-01,  6.72641158e-01,\n",
       "           3.40887159e-02,  1.96681008e-01,  3.32069993e-01,\n",
       "           5.18067956e-01,  1.04460359e+00,  4.56062078e-01,\n",
       "           2.61725456e-01,  5.75585783e-01,  2.71887213e-01,\n",
       "           1.98809683e-01,  8.62057626e-01,  3.42595547e-01,\n",
       "           5.80603421e-01,  7.32531667e-01,  7.42925227e-01,\n",
       "           1.18085706e+00,  6.21984661e-01,  2.02956215e-01,\n",
       "           1.96060240e-01, -1.35519832e-01, -4.08501625e-02,\n",
       "          -5.07392883e-01, -3.61381412e-01],\n",
       "         [-1.06058374e-01,  5.95110178e-01,  4.28310269e-03,\n",
       "          -5.18237948e-01, -5.51881194e-01,  3.06386590e-01,\n",
       "           1.50878072e-01,  5.29339790e-01,  3.29761326e-01,\n",
       "           7.90486276e-01,  5.05057216e-01,  1.51467636e-01,\n",
       "           5.33966959e-01,  1.93225041e-01,  8.24892163e-01,\n",
       "           5.75716496e-01,  5.72912395e-01,  6.03487007e-02,\n",
       "           7.51881301e-01, -1.65228739e-01,  3.83829176e-01,\n",
       "           1.86884701e-01,  4.72768694e-01,  8.84366810e-01,\n",
       "           1.59707949e-01,  2.00867847e-01, -2.98221279e-02,\n",
       "           1.35780990e-01, -2.21507251e-01, -1.78798765e-01,\n",
       "           2.04386264e-01,  1.50805160e-01],\n",
       "         [ 1.78931415e-01,  3.95215839e-01,  8.06451589e-02,\n",
       "          -4.91087884e-01, -4.12606657e-01,  3.32997739e-01,\n",
       "           4.59242612e-03,  5.02371192e-01,  2.73653567e-01,\n",
       "           7.88663208e-01,  5.49732864e-01,  8.03483427e-02,\n",
       "           8.67273271e-01,  5.62079966e-01,  3.39091234e-02,\n",
       "           6.59986794e-01, -1.26723751e-01,  2.87398010e-01,\n",
       "           1.69165522e-01,  4.74214673e-01,  7.43691862e-01,\n",
       "           2.62009919e-01, -9.22714476e-04,  1.34123117e-01,\n",
       "           1.50197044e-01, -1.13191590e-01, -7.84341916e-02,\n",
       "           2.53413528e-01, -5.73513269e-01,  2.49526709e-01,\n",
       "           7.13579953e-01,  8.00953582e-02]], dtype=float32),\n",
       "  array([[ 0.06772858, -0.84322923,  0.88891345, -1.6484071 , -0.34553805,\n",
       "          -0.70521986,  0.25301734, -0.56876427, -1.1125365 ,  0.23267925,\n",
       "          -0.2838908 , -0.35224155,  0.93925375,  0.01903042,  0.6487261 ,\n",
       "          -0.23201995,  0.12373082, -0.26906085, -0.4524621 ,  0.9761932 ,\n",
       "          -1.3817466 , -0.03656138, -0.7128789 ,  0.30850235,  0.16006583,\n",
       "          -0.00567151, -0.29696274, -0.2852122 , -0.16919954, -0.23460476,\n",
       "          -0.02225418, -0.26817498],\n",
       "         [-0.02613476, -0.9810475 , -0.40576497, -0.14037646, -0.71665114,\n",
       "          -0.06561633, -0.49675557, -0.19822977, -0.01292498, -1.0211791 ,\n",
       "           0.09474035, -0.7746159 , -0.7124997 , -0.38906297, -0.74657756,\n",
       "          -1.1999291 , -0.27648658,  0.15342425, -0.8387967 , -0.16387095,\n",
       "           0.33270875, -0.43957824, -0.2138849 , -0.35450196, -0.77310985,\n",
       "          -0.31554428, -0.56158316, -0.39647993, -0.4877595 , -0.654433  ,\n",
       "          -0.09969018, -0.7577406 ],\n",
       "         [ 0.19663768, -0.8468073 , -0.31220955, -0.17815225, -0.67327845,\n",
       "          -0.08988103, -0.5915096 , -0.07594179,  0.00892548, -0.96642876,\n",
       "           0.29090872, -0.6740272 , -0.9213586 , -0.16951908,  0.18171574,\n",
       "          -0.9074101 , -0.0469621 ,  0.39108208, -0.58496135, -0.14376692,\n",
       "          -0.33773726, -0.62375486, -0.4502099 , -0.7379956 , -0.53366184,\n",
       "          -0.77002484, -0.69844913, -0.03068519,  0.43519562, -0.24978435,\n",
       "          -0.21473597, -0.6751834 ]], dtype=float32)))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output.predictions), output.predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì˜ë³´ë©´ predictionsê°€ ì¼ë°˜ì ì¸ í…ì„œê°€ ì•„ë‹ˆë¼ íŠœí”Œí˜•íƒœë¡œ ì œê³µë˜ê³  ê¸¸ì´ê°€ 2ë¼ëŠ” ì ì…ë‹ˆë‹¤. Extraction-based MRCì˜ íŠ¹ì„±ì„ ìƒê°í•´ë³´ë©´ Startì§€ì ê³¼ Endì§€ì ì„ ì˜ˆì¸¡í•´ì•¼í•˜ë‹ˆ ì–´ì°Œë³´ë©´ ë‹¹ì—°í•©ë‹ˆë‹¤. lossëŠ” ì• ì´ˆì— datasetsì—ë‹¤ê°€ í† í° ìœ„ì¹˜ë¥¼ ë„£ì–´ì¤¬ìœ¼ë‹ˆ ê³„ì‚°í•˜ëŠ” ê²ƒì€ BCEë¡œ ì‰½ê²Œ ê³„ì‚°í–ˆì„ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ë§Œ ì‹¤ì œë¡œ ì •ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì„œ lossê°€ ì•„ë‹Œ ë‹¤ë¥¸ metricì„ ì¸¡ì •í•˜ëŠ” ê²ƒì€ ë˜ ë‹¤ë¥¸ ë¬¸ì œì¸ë°ìš”, ì–´ë–»ê²Œ ì²˜ë¦¬í–ˆëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51984efefe8145a98f502ac58b0d056f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if self.post_process_function is not None and self.compute_metrics is not None:\n",
    "    eval_preds = self.post_process_function(\n",
    "        eval_examples, eval_dataset, output.predictions, self.args\n",
    "    )\n",
    "    metrics = self.compute_metrics(eval_preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì—¬ê¸°ì„œ ì‚¬ìš©ëœ `self.post_process_function`ì€ ë² ì´ìŠ¤ë¼ì¸ `train.py` ë‚´ì— ìˆëŠ” `post_processing_function`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì¸ë°ìš”, ì´ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ë„ ë” ì¤‘ìš”í•œ ê±´ ì´ ì•ˆì—ì„œ ì‚¬ìš©ë˜ëŠ” `utils_qa.py` ë‚´ì˜ `postprocess_qa_predictions` í•¨ìˆ˜ì…ë‹ˆë‹¤. ì¼ë‹¨ ê²°ê³¼ë¶€í„° ì‚´í´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'ex-1', 'answers': {'answer_start': [24], 'text': ['ì„¸ê³„ 4ìœ„']}},\n",
       " {'id': 'ex-2', 'answers': {'answer_start': [11], 'text': ['190ê°œêµ­']}}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_preds.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'ex-1', 'prediction_text': 'êµ­ì œì ì¸ ë„ì‹œì´ë‹¤. ì„œìš¸ì˜'},\n",
       " {'id': 'ex-2', 'prediction_text': 'ì„¸ê³„ 190ê°œêµ­ ì´ìƒì—ì„œ ì´ìš© ì¤‘ì´ë‹¤. ìŠ¤ë¬¼ë‹¤ì„¯ìŠ¤ë¬¼í•˜ë‚˜ ì¬ë°Œ'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_preds.predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„°ëŠ” 3ê°œë¥¼ ì œê³µí–ˆëŠ”ë° ì˜ ê³ ìœ  id ê°œìˆ˜ì— ë§ì¶°ì„œ ê²°ê³¼ë¥¼ ë³€í™˜í•´ì¤¬ë„¤ìš”. ì´ ëª¨ë“  ê²Œ `postprocess_qa_predictions` ì•ˆì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼ì…ë‹ˆë‹¤. ì„¸ì„¸í•˜ê²Œ ëª¨ë“  ê³¼ì •ì„ ë‹¤ ë³´ì§€ëŠ” ì•ŠëŠ” ëŒ€ì‹ ì— ì´ í•¨ìˆ˜ì— ì œê³µë˜ëŠ” ì…ë ¥ê°’ë“¤ì„ ì•Œê³  ìˆìœ¼ë©´ ì–´ëŠ ì •ë„ í•¨ìˆ˜ ë‚´ì˜ íë¦„ì„ ì´í•´í•˜ëŠ”ë° ë„ì›€ì´ ë˜ê¸° ë•Œë¬¸ì— ì‘ì„±í•´ë³´ì•˜ìŠµë‹ˆë‹¤. ì–´ë–¤ ìë£Œí˜•ì´ ë“¤ì–´ê°€ëŠ”ì§€ ì´í•´í•˜ë©´ ì €í¬ê°€ `utils_qa.py` ë‚´ì— ìì„¸í•˜ê²Œ í•œê¸€ë¡œ ì£¼ì„ì„ ë‚¨ê²¨ë†“ì•˜ê¸° ë•Œë¬¸ì— ì´í•´í•˜ëŠ”ë° ì¡°ê¸ˆì´ë‚˜ë§ˆ ë” ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤!\n",
    "\n",
    "```python\n",
    "def postprocess_qa_predictions(\n",
    "    examples,                                   # <- eval_examples\n",
    "    features,                                   # <- eval_dataset\n",
    "    predictions: Tuple[np.ndarray, np.ndarray], # <- output.predictions\n",
    "    version_2_with_negative: bool = False,\n",
    "    n_best_size: int = 20,\n",
    "    max_answer_length: int = 30,\n",
    "    null_score_diff_threshold: float = 0.0,\n",
    "    output_dir: Optional[str] = None,\n",
    "    prefix: Optional[str] = None,\n",
    "    is_world_process_zero: bool = True,\n",
    "):\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eval_examples`ì™€ `eval_dataset`ì˜ ì°¨ì´ëŠ” ì „ì²˜ë¦¬ ì°¨ì´ì…ë‹ˆë‹¤. `eval_examples`ëŠ” ì½”ë“œë¥¼ í™•ì¸í•´ë³´ë©´ ì „ì²˜ë¦¬ê°€ ë˜ê¸° ì „ì˜ `dataset[\"validation\"]`ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤. `eval_dataset`ì€ `preprocess_validation_features` í•¨ìˆ˜ë¥¼ ê±°ì¹œ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'question', 'answers', 'title', 'id', 'document_id'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`output.predictions`ëŠ” start_token í™•ë¥ ê³¼ end_token í™•ë¥ ì„ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” logitê°’ì„ í¬í•¨í•©ë‹ˆë‹¤. ì²«ë²ˆì§¸ í–‰ë ¬ì€ start_token, ë‘ë²ˆì§¸ í–‰ë ¬ì€ end_tokenì˜ ê°’ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-2.27704510e-01,  2.16861933e-01,  9.41800296e-01,\n",
       "          4.41306651e-01,  2.32540250e-01,  8.80979180e-01,\n",
       "         -4.31050867e-01, -1.55686215e-01,  6.72641158e-01,\n",
       "          3.40887159e-02,  1.96681008e-01,  3.32069993e-01,\n",
       "          5.18067956e-01,  1.04460359e+00,  4.56062078e-01,\n",
       "          2.61725456e-01,  5.75585783e-01,  2.71887213e-01,\n",
       "          1.98809683e-01,  8.62057626e-01,  3.42595547e-01,\n",
       "          5.80603421e-01,  7.32531667e-01,  7.42925227e-01,\n",
       "          1.18085706e+00,  6.21984661e-01,  2.02956215e-01,\n",
       "          1.96060240e-01, -1.35519832e-01, -4.08501625e-02,\n",
       "         -5.07392883e-01, -3.61381412e-01],\n",
       "        [-1.06058374e-01,  5.95110178e-01,  4.28310269e-03,\n",
       "         -5.18237948e-01, -5.51881194e-01,  3.06386590e-01,\n",
       "          1.50878072e-01,  5.29339790e-01,  3.29761326e-01,\n",
       "          7.90486276e-01,  5.05057216e-01,  1.51467636e-01,\n",
       "          5.33966959e-01,  1.93225041e-01,  8.24892163e-01,\n",
       "          5.75716496e-01,  5.72912395e-01,  6.03487007e-02,\n",
       "          7.51881301e-01, -1.65228739e-01,  3.83829176e-01,\n",
       "          1.86884701e-01,  4.72768694e-01,  8.84366810e-01,\n",
       "          1.59707949e-01,  2.00867847e-01, -2.98221279e-02,\n",
       "          1.35780990e-01, -2.21507251e-01, -1.78798765e-01,\n",
       "          2.04386264e-01,  1.50805160e-01],\n",
       "        [ 1.78931415e-01,  3.95215839e-01,  8.06451589e-02,\n",
       "         -4.91087884e-01, -4.12606657e-01,  3.32997739e-01,\n",
       "          4.59242612e-03,  5.02371192e-01,  2.73653567e-01,\n",
       "          7.88663208e-01,  5.49732864e-01,  8.03483427e-02,\n",
       "          8.67273271e-01,  5.62079966e-01,  3.39091234e-02,\n",
       "          6.59986794e-01, -1.26723751e-01,  2.87398010e-01,\n",
       "          1.69165522e-01,  4.74214673e-01,  7.43691862e-01,\n",
       "          2.62009919e-01, -9.22714476e-04,  1.34123117e-01,\n",
       "          1.50197044e-01, -1.13191590e-01, -7.84341916e-02,\n",
       "          2.53413528e-01, -5.73513269e-01,  2.49526709e-01,\n",
       "          7.13579953e-01,  8.00953582e-02]], dtype=float32),\n",
       " array([[ 0.06772858, -0.84322923,  0.88891345, -1.6484071 , -0.34553805,\n",
       "         -0.70521986,  0.25301734, -0.56876427, -1.1125365 ,  0.23267925,\n",
       "         -0.2838908 , -0.35224155,  0.93925375,  0.01903042,  0.6487261 ,\n",
       "         -0.23201995,  0.12373082, -0.26906085, -0.4524621 ,  0.9761932 ,\n",
       "         -1.3817466 , -0.03656138, -0.7128789 ,  0.30850235,  0.16006583,\n",
       "         -0.00567151, -0.29696274, -0.2852122 , -0.16919954, -0.23460476,\n",
       "         -0.02225418, -0.26817498],\n",
       "        [-0.02613476, -0.9810475 , -0.40576497, -0.14037646, -0.71665114,\n",
       "         -0.06561633, -0.49675557, -0.19822977, -0.01292498, -1.0211791 ,\n",
       "          0.09474035, -0.7746159 , -0.7124997 , -0.38906297, -0.74657756,\n",
       "         -1.1999291 , -0.27648658,  0.15342425, -0.8387967 , -0.16387095,\n",
       "          0.33270875, -0.43957824, -0.2138849 , -0.35450196, -0.77310985,\n",
       "         -0.31554428, -0.56158316, -0.39647993, -0.4877595 , -0.654433  ,\n",
       "         -0.09969018, -0.7577406 ],\n",
       "        [ 0.19663768, -0.8468073 , -0.31220955, -0.17815225, -0.67327845,\n",
       "         -0.08988103, -0.5915096 , -0.07594179,  0.00892548, -0.96642876,\n",
       "          0.29090872, -0.6740272 , -0.9213586 , -0.16951908,  0.18171574,\n",
       "         -0.9074101 , -0.0469621 ,  0.39108208, -0.58496135, -0.14376692,\n",
       "         -0.33773726, -0.62375486, -0.4502099 , -0.7379956 , -0.53366184,\n",
       "         -0.77002484, -0.69844913, -0.03068519,  0.43519562, -0.24978435,\n",
       "         -0.21473597, -0.6751834 ]], dtype=float32))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`postprocess_qa_predictions`ëŠ” ë¡œì§ ìì²´ë„ ê·¸ë ‡ê²Œ ë³µì¡í•˜ì§€ ì•Šìœ¼ë‚˜ ì–´ë–¤ ë°ì´í„°ê°€ ë“¤ì–´ê°€ëŠ”ì§€ ëª°ë¼ì„œ ë”°ë¼ê°€ê¸°ê°€ í˜ë“ ë°ìš”, ì—¬ê¸°ì„œ ì‹œì‘í•˜ì‹œë©´ ì¡°ê¸ˆ ë” í¸í•˜ì‹¤ê±°ë¼ ì˜ˆìƒí•©ë‹ˆë‹¤ :) \n",
    "\n",
    "LLMì˜ ì‹œëŒ€ë¡œ ì ‘ì–´ë“¤ë©´ì„œ ëª¨ë¸ì˜ ê·œëª¨ê°€ ì»¤ì§„ ê²ƒë„ ì¤‘ìš”í•˜ì§€ë§Œ, ì €ëŠ” ê·¸ë§Œí¼ ì¢‹ì€ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì£¼ëŠ” ê²ƒì´ êµ‰ì¥íˆ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë„ˆë¬´ ì¡°ê¸‰í•˜ê²Œ ìƒê°í•˜ì§€ ë§ˆì‹œê³  ì´ë²ˆ MRCì—ì„œëŠ” Huggingface ì‚¬ìš©ë°©ë²•ê³¼ ì—¬ê¸°ì— ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í—¤ì„œ ë„£ì–´ì£¼ëŠ” ë°©ë²•ë“¤ì— ëŒ€í•´ ë§ì´ ê³µë¶€í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ 20000\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ì½˜í…ì¸  ë¼ì´ì„ ìŠ¤**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **ë³¸ êµìœ¡ ì½˜í…ì¸ ì˜ ì§€ì‹ì¬ì‚°ê¶Œì€ ì¬ë‹¨ë²•ì¸ ë„¤ì´ë²„ì»¤ë„¥íŠ¸ì— ê·€ì†ë©ë‹ˆë‹¤. ë³¸ ì½˜í…ì¸ ë¥¼ ì–´ë– í•œ ê²½ë¡œë¡œë“  ì™¸ë¶€ë¡œ ìœ ì¶œ ë° ìˆ˜ì •í•˜ëŠ” í–‰ìœ„ë¥¼ ì—„ê²©íˆ ê¸ˆí•©ë‹ˆë‹¤.** ë‹¤ë§Œ, ë¹„ì˜ë¦¬ì  êµìœ¡ ë° ì—°êµ¬í™œë™ì— í•œì •ë˜ì–´ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜ ì¬ë‹¨ì˜ í—ˆë½ì„ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„ë°˜í•˜ëŠ” ê²½ìš°, ê´€ë ¨ ë²•ë¥ ì— ë”°ë¼ ì±…ì„ì„ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ë¼ì´ì„ ìŠ¤ : MIT License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
